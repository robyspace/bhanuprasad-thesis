{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Improved CNN-LSTM Model for Network Intrusion Detection\n",
    "\n",
    "## CSE-CIC-IDS-2018 Dataset\n",
    "\n",
    "### ðŸ“Œ What's Different in This Implementation?\n",
    "\n",
    "This notebook fixes critical issues in the original CNN-LSTM implementation:\n",
    "\n",
    "#### âœ… Fixed Issues:\n",
    "1. **Conv1D instead of Conv2D** - Appropriate for sequential tabular data\n",
    "2. **Proper sequence creation** - Sliding window with configurable stride\n",
    "3. **Reduced dropout** - Better learning without over-regularization\n",
    "4. **Better architecture** - Three variants to test (CNN-LSTM v1, v2, LSTM-only)\n",
    "5. **Improved training strategy** - Better callbacks and hyperparameters\n",
    "\n",
    "#### ðŸ“Š Expected Performance:\n",
    "- **Target Accuracy**: 75-85% (vs 51% in original)\n",
    "- **Target ROC-AUC**: 0.80-0.90 (vs 0.50 in original)\n",
    "- **Training Time**: ~15-25 minutes on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if needed)\n",
    "# !pip install tensorflow pandas numpy scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks, regularizers\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, roc_curve\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import time\n",
    "import pickle\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"IMPROVED CNN-LSTM MODEL FOR INTRUSION DETECTION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU'))} GPU(s)\")\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU devices:\", tf.config.list_physical_devices('GPU'))\n",
    "else:\n",
    "    print(\"âš ï¸  No GPU detected. Training will be slower on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ Step 2: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# UPDATE THESE PATHS TO MATCH YOUR GOOGLE DRIVE STRUCTURE\n",
    "\n",
    "PROJECT_DIR = '/content/drive/MyDrive/IDS_Research'\n",
    "MODEL_DIR = f'{PROJECT_DIR}/models'\n",
    "RESULTS_DIR = f'{PROJECT_DIR}/results'\n",
    "\n",
    "# Model hyperparameters\n",
    "TIME_STEPS = 10        # Number of time steps to look back\n",
    "BATCH_SIZE = 256       # Batch size for training\n",
    "EPOCHS = 30            # Maximum epochs\n",
    "LEARNING_RATE = 0.001  # Initial learning rate\n",
    "\n",
    "# Choose architecture: 'v1', 'v2', or 'lstm_only'\n",
    "ARCHITECTURE = 'v1'  # Start with v1, can try others later\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Time steps: {TIME_STEPS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Architecture: {ARCHITECTURE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 3: Load Preprocessed Data\n",
    "\n",
    "**Important:** This notebook assumes you've already run your data preprocessing.\n",
    "\n",
    "You should have:\n",
    "- `X_train_scaled`, `y_train`\n",
    "- `X_val_scaled`, `y_val`  \n",
    "- `X_test_scaled`, `y_test`\n",
    "\n",
    "If you haven't done this yet, run your ML_IDS_v4.ipynb notebook up to the preprocessing section first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if preprocessed data exists\n",
    "try:\n",
    "    print(\"Checking preprocessed data...\")\n",
    "    print(f\"X_train_scaled shape: {X_train_scaled.shape}\")\n",
    "    print(f\"X_val_scaled shape: {X_val_scaled.shape}\")\n",
    "    print(f\"X_test_scaled shape: {X_test_scaled.shape}\")\n",
    "    print(f\"\\nLabel distributions:\")\n",
    "    print(f\"  Train: {np.bincount(y_train)}\")\n",
    "    print(f\"  Val:   {np.bincount(y_val)}\")\n",
    "    print(f\"  Test:  {np.bincount(y_test)}\")\n",
    "    print(\"\\nâœ“ Data loaded successfully!\")\nexcept NameError:\n",
    "    print(\"âŒ Error: Preprocessed data not found!\")\n",
    "    print(\"\\nPlease run your data preprocessing first.\")\n",
    "    print(\"You can either:\")\n",
    "    print(\"1. Run ML_IDS_v4.ipynb up to the preprocessing section\")\n",
    "    print(\"2. Or copy the preprocessing code from that notebook here\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ Step 4: Create Optimized Sequences\n",
    "\n",
    "This is where we fix the temporal sequence creation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimized_sequences(X, y, time_steps=10, stride=1):\n",
    "    \"\"\"\n",
    "    Create sequences with sliding window approach\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix (numpy array or DataFrame)\n",
    "        y: Labels (pandas Series or numpy array)\n",
    "        time_steps: Number of time steps to look back\n",
    "        stride: Step size for sliding window (1=no skip, 5=skip 4)\n",
    "    \n",
    "    Returns:\n",
    "        X_seq: Sequential data (samples, time_steps, features)\n",
    "        y_seq: Labels for sequences\n",
    "    \"\"\"\n",
    "    print(f\"\\nCreating sequences with time_steps={time_steps}, stride={stride}...\")\n",
    "    \n",
    "    # Convert to numpy if needed\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.values\n",
    "    if isinstance(y, pd.Series):\n",
    "        y = y.values\n",
    "    \n",
    "    Xs, ys = [], []\n",
    "    \n",
    "    # Sliding window approach\n",
    "    for i in range(0, len(X) - time_steps, stride):\n",
    "        sequence = X[i:(i + time_steps)]\n",
    "        label = y[i + time_steps]\n",
    "        Xs.append(sequence)\n",
    "        ys.append(label)\n",
    "    \n",
    "    X_seq = np.array(Xs)\n",
    "    y_seq = np.array(ys)\n",
    "    \n",
    "    print(f\"âœ“ Created {len(X_seq):,} sequences\")\n",
    "    print(f\"  Shape: {X_seq.shape}\")\n",
    "    print(f\"  Label distribution: {np.bincount(y_seq.astype(int))}\")\n",
    "    \n",
    "    return X_seq, y_seq\n",
    "\n",
    "# Create sequences\n",
    "# Using stride=5 for train/val to reduce overlap and speed up training\n",
    "# Using stride=1 for test to evaluate on all data\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CREATING SEQUENTIAL DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X_train_seq, y_train_seq = create_optimized_sequences(\n",
    "    X_train_scaled, y_train, time_steps=TIME_STEPS, stride=5\n",
    ")\n",
    "\n",
    "X_val_seq, y_val_seq = create_optimized_sequences(\n",
    "    X_val_scaled, y_val, time_steps=TIME_STEPS, stride=5\n",
    ")\n",
    "\n",
    "X_test_seq, y_test_seq = create_optimized_sequences(\n",
    "    X_test_scaled, y_test, time_steps=TIME_STEPS, stride=1\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ All sequences created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Step 5: Build Improved Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_improved_cnn_lstm_v1(input_shape):\n",
    "    \"\"\"\n",
    "    Improved CNN-LSTM using Conv1D\n",
    "    \n",
    "    Key improvements:\n",
    "    - Conv1D instead of Conv2D (appropriate for sequential data)\n",
    "    - Reduced dropout (was 110%, now 30%)\n",
    "    - Better regularization with L2\n",
    "    - Streamlined architecture\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Input\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        # CNN Block 1 - Local feature extraction\n",
    "        layers.Conv1D(64, kernel_size=3, activation='relu', padding='same',\n",
    "                     kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # CNN Block 2\n",
    "        layers.Conv1D(128, kernel_size=3, activation='relu', padding='same',\n",
    "                     kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # LSTM Block - Temporal dependencies\n",
    "        layers.LSTM(128, return_sequences=True,\n",
    "                   kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.LSTM(64, return_sequences=False,\n",
    "                   kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Classification layers\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.1),\n",
    "        \n",
    "        # Output\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def build_improved_cnn_lstm_v2(input_shape):\n",
    "    \"\"\"\n",
    "    Simplified CNN-LSTM - lighter architecture\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        # Simple CNN\n",
    "        layers.Conv1D(64, kernel_size=3, activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # LSTM\n",
    "        layers.LSTM(128, return_sequences=True),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.LSTM(64, return_sequences=False),\n",
    "        \n",
    "        # Classification\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def build_lstm_only(input_shape):\n",
    "    \"\"\"\n",
    "    LSTM-only model (no CNN)\n",
    "    May perform better on tabular data\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        layers.Bidirectional(layers.LSTM(128, return_sequences=True)),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.Bidirectional(layers.LSTM(64, return_sequences=False)),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Classification\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Build selected architecture\n",
    "print(\"=\"*80)\n",
    "print(\"BUILDING MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "input_shape = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "print(f\"Architecture: {ARCHITECTURE}\")\n",
    "\n",
    "if ARCHITECTURE == 'v1':\n",
    "    model = build_improved_cnn_lstm_v1(input_shape)\n",
    "elif ARCHITECTURE == 'v2':\n",
    "    model = build_improved_cnn_lstm_v2(input_shape)\n",
    "elif ARCHITECTURE == 'lstm_only':\n",
    "    model = build_lstm_only(input_shape)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown architecture: {ARCHITECTURE}\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Step 6: Compile Model with Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights for imbalanced data\n",
    "class_weights_array = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train_seq),\n",
    "    y=y_train_seq\n",
    ")\n",
    "class_weights = {\n",
    "    0: class_weights_array[0],\n",
    "    1: class_weights_array[1]\n",
    "}\n",
    "\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "print(f\"This helps handle imbalanced data (more benign than attack samples)\")\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc')\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Model compiled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Step 7: Setup Training Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callbacks for better training\n",
    "callbacks_list = [\n",
    "    # Early stopping - stop if no improvement\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_auc',\n",
    "        patience=7,\n",
    "        restore_best_weights=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Reduce learning rate when stuck\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Save best model\n",
    "    callbacks.ModelCheckpoint(\n",
    "        f'{MODEL_DIR}/improved_cnn_lstm_best.h5',\n",
    "        monitor='val_auc',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # TensorBoard logging\n",
    "    callbacks.TensorBoard(\n",
    "        log_dir=f'{PROJECT_DIR}/logs',\n",
    "        histogram_freq=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"âœ“ Callbacks configured:\")\n",
    "print(\"  - Early stopping (patience=7)\")\n",
    "print(\"  - Learning rate reduction (patience=3)\")\n",
    "print(\"  - Model checkpoint (save best)\")\n",
    "print(\"  - TensorBoard logging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 8: Train Model\n",
    "\n",
    "This will take 15-25 minutes on GPU, longer on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TRAINING MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Training samples: {len(X_train_seq):,}\")\n",
    "print(f\"Validation samples: {len(X_val_seq):,}\")\n",
    "print(\"\\nTraining started...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"âœ“ TRAINING COMPLETED!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "print(f\"Final training accuracy: {history.history['accuracy'][-1]*100:.2f}%\")\n",
    "print(f\"Final validation accuracy: {history.history['val_accuracy'][-1]*100:.2f}%\")\n",
    "print(f\"Best validation AUC: {max(history.history['val_auc']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 9: Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comprehensive training history\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Improved CNN-LSTM Training History', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics = ['loss', 'accuracy', 'precision', 'recall', 'auc']\n",
    "titles = ['Loss', 'Accuracy', 'Precision', 'Recall', 'AUC']\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    ax.plot(history.history[metric], label=f'Training {title}', linewidth=2)\n",
    "    ax.plot(history.history[f'val_{metric}'], label=f'Validation {title}', linewidth=2)\n",
    "    ax.set_title(f'Model {title}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate plot\n",
    "if 'lr' in history.history:\n",
    "    ax = axes[1, 2]\n",
    "    ax.plot(history.history['lr'], label='Learning Rate', linewidth=2, color='red')\n",
    "    ax.set_title('Learning Rate Schedule', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Learning Rate')\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/improved_cnn_lstm_training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Training history visualized and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Step 10: Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Make predictions\n",
    "start_time = time.time()\n",
    "y_pred_proba = model.predict(X_test_seq, verbose=0)\n",
    "inference_time = time.time() - start_time\n",
    "\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "y_pred_proba = y_pred_proba.flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test_seq, y_pred)\n",
    "precision = precision_score(y_test_seq, y_pred, zero_division=0)\n",
    "recall = recall_score(y_test_seq, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test_seq, y_pred, zero_division=0)\n",
    "roc_auc = roc_auc_score(y_test_seq, y_pred_proba)\n",
    "\n",
    "# Confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_seq, y_pred).ravel()\n",
    "fpr_value = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "# Latency\n",
    "avg_latency = (inference_time / len(X_test_seq)) * 1000  # ms\n",
    "\n",
    "print(f\"\\nImproved CNN-LSTM Test Performance:\")\n",
    "print(f\"  Accuracy:  {accuracy*100:.2f}%\")\n",
    "print(f\"  Precision: {precision*100:.2f}%\")\n",
    "print(f\"  Recall:    {recall*100:.2f}%\")\n",
    "print(f\"  F1-Score:  {f1*100:.2f}%\")\n",
    "print(f\"  ROC-AUC:   {roc_auc:.4f}\")\n",
    "print(f\"  FPR:       {fpr_value*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  TN: {tn:>6,}  FP: {fp:>6,}\")\n",
    "print(f\"  FN: {fn:>6,}  TP: {tp:>6,}\")\n",
    "\n",
    "print(f\"\\nInference Performance:\")\n",
    "print(f\"  Avg Latency: {avg_latency:.2f} ms/sample\")\n",
    "print(f\"  Total Time:  {inference_time:.2f} seconds\")\n",
    "print(f\"  Throughput:  {len(X_test_seq)/inference_time:.2f} samples/sec\")\n",
    "\n",
    "# Comparison with original\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON WITH ORIGINAL CNN-LSTM\")\n",
    "print(\"=\"*80)\n",
    "original_acc = 0.510\n",
    "original_auc = 0.501\n",
    "improvement_acc = ((accuracy - original_acc) / original_acc) * 100\n",
    "improvement_auc = ((roc_auc - original_auc) / original_auc) * 100\n",
    "\n",
    "print(f\"  Accuracy:  {original_acc*100:.2f}% â†’ {accuracy*100:.2f}% ({improvement_acc:+.1f}% improvement)\")\n",
    "print(f\"  ROC-AUC:   {original_auc:.3f} â†’ {roc_auc:.3f} ({improvement_auc:+.1f}% improvement)\")\n",
    "\n",
    "# Store results\n",
    "results = {\n",
    "    'accuracy': float(accuracy),\n",
    "    'precision': float(precision),\n",
    "    'recall': float(recall),\n",
    "    'f1_score': float(f1),\n",
    "    'roc_auc': float(roc_auc),\n",
    "    'fpr': float(fpr_value),\n",
    "    'confusion_matrix': {'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp)},\n",
    "    'avg_latency_ms': float(avg_latency),\n",
    "    'training_time_seconds': float(training_time)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Step 11: Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test_seq, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Benign', 'Attack'],\n",
    "            yticklabels=['Benign', 'Attack'])\n",
    "plt.title('Improved CNN-LSTM Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/improved_cnn_lstm_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test_seq, y_pred_proba)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2.5,\n",
    "         label=f'Improved CNN-LSTM (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curve - Improved CNN-LSTM', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/improved_cnn_lstm_roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Visualizations saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Step 12: Save Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SAVING MODEL AND RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save final model\n",
    "model_path = f'{MODEL_DIR}/improved_cnn_lstm_ids_model.h5'\n",
    "model.save(model_path)\n",
    "print(f\"âœ“ Model saved to: {model_path}\")\n",
    "\n",
    "# Save results as JSON\n",
    "results_dict = {\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'dataset': 'CSE-CIC-IDS-2018',\n",
    "    'architecture': ARCHITECTURE,\n",
    "    'time_steps': TIME_STEPS,\n",
    "    'config': {\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'epochs': EPOCHS,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "    },\n",
    "    'results': results,\n",
    "    'training_samples': int(len(X_train_seq)),\n",
    "    'validation_samples': int(len(X_val_seq)),\n",
    "    'test_samples': int(len(X_test_seq)),\n",
    "    'comparison_with_original': {\n",
    "        'original_accuracy': 0.510,\n",
    "        'improved_accuracy': float(accuracy),\n",
    "        'accuracy_improvement_percent': float(improvement_acc),\n",
    "        'original_auc': 0.501,\n",
    "        'improved_auc': float(roc_auc),\n",
    "        'auc_improvement_percent': float(improvement_auc)\n",
    "    }\n",
    "}\n",
    "\n",
    "results_path = f'{RESULTS_DIR}/improved_cnn_lstm_results.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results_dict, f, indent=4)\n",
    "\n",
    "print(f\"âœ“ Results saved to: {results_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… ALL DONE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nYour improved CNN-LSTM model is ready for deployment!\")\n",
    "print(f\"\\nKey files saved:\")\n",
    "print(f\"  1. Model: {model_path}\")\n",
    "print(f\"  2. Results: {results_path}\")\n",
    "print(f\"  3. Visualizations: {RESULTS_DIR}/improved_cnn_lstm_*.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¬ Step 13: Optional - Compare All Three Architectures\n",
    "\n",
    "Run this cell to test all three architectures and compare them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run full comparison\n",
    "\n",
    "# print(\"=\"*80)\n",
    "# print(\"COMPARING ALL ARCHITECTURES\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# architectures = ['v1', 'v2', 'lstm_only']\n",
    "# comparison_results = {}\n",
    "\n",
    "# for arch in architectures:\n",
    "#     print(f\"\\n{'='*80}\")\n",
    "#     print(f\"Testing: {arch}\")\n",
    "#     print(f\"{'='*80}\")\n",
    "#     \n",
    "#     # Build model\n",
    "#     if arch == 'v1':\n",
    "#         test_model = build_improved_cnn_lstm_v1(input_shape)\n",
    "#     elif arch == 'v2':\n",
    "#         test_model = build_improved_cnn_lstm_v2(input_shape)\n",
    "#     else:\n",
    "#         test_model = build_lstm_only(input_shape)\n",
    "#     \n",
    "#     # Compile\n",
    "#     test_model.compile(\n",
    "#         optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "#         loss='binary_crossentropy',\n",
    "#         metrics=['accuracy', keras.metrics.AUC(name='auc')]\n",
    "#     )\n",
    "#     \n",
    "#     # Train\n",
    "#     hist = test_model.fit(\n",
    "#         X_train_seq, y_train_seq,\n",
    "#         batch_size=BATCH_SIZE,\n",
    "#         epochs=20,  # Reduced epochs for comparison\n",
    "#         validation_data=(X_val_seq, y_val_seq),\n",
    "#         class_weight=class_weights,\n",
    "#         callbacks=[callbacks.EarlyStopping(monitor='val_auc', patience=5, restore_best_weights=True)],\n",
    "#         verbose=0\n",
    "#     )\n",
    "#     \n",
    "#     # Evaluate\n",
    "#     y_pred_proba = test_model.predict(X_test_seq, verbose=0)\n",
    "#     y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "#     \n",
    "#     acc = accuracy_score(y_test_seq, y_pred)\n",
    "#     auc = roc_auc_score(y_test_seq, y_pred_proba)\n",
    "#     \n",
    "#     comparison_results[arch] = {'accuracy': acc, 'auc': auc}\n",
    "#     print(f\"  Accuracy: {acc*100:.2f}%, AUC: {auc:.4f}\")\n",
    "\n",
    "# # Plot comparison\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "# \n",
    "# archs = list(comparison_results.keys())\n",
    "# accs = [comparison_results[a]['accuracy'] for a in archs]\n",
    "# aucs = [comparison_results[a]['auc'] for a in archs]\n",
    "# \n",
    "# ax1.bar(archs, accs, color=['blue', 'green', 'orange'])\n",
    "# ax1.set_title('Accuracy Comparison')\n",
    "# ax1.set_ylabel('Accuracy')\n",
    "# ax1.set_ylim([0, 1])\n",
    "# \n",
    "# ax2.bar(archs, aucs, color=['blue', 'green', 'orange'])\n",
    "# ax2.set_title('AUC Comparison')\n",
    "# ax2.set_ylabel('AUC')\n",
    "# ax2.set_ylim([0, 1])\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# plt.savefig(f'{RESULTS_DIR}/architecture_comparison.png', dpi=300)\n",
    "# plt.show()\n",
    "# \n",
    "# print(\"\\nâœ“ Architecture comparison complete!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
